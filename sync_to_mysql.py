# sync_to_mysql.py

import os
import sys
import time as pytime
import logging
import random
import pandas as pd
import akshare as ak
import requests
from datetime import datetime, timedelta, time
from sqlalchemy import create_engine, text
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# ================== é…ç½® ==================
MYSQL_USER = os.getenv("MYSQL_USER", "root")
MYSQL_PASSWORD = os.getenv("MYSQL_PASSWORD", "InsightOne123456")
MYSQL_HOST = os.getenv("MYSQL_HOST", "127.0.0.1")
MYSQL_PORT = os.getenv("MYSQL_PORT", "3306")
MYSQL_DB = os.getenv("MYSQL_DB", "stock_db")
CODE_CSV_PATH = os.getenv("CODE_CSV_PATH", "./code.csv")
MAX_INCREMENTAL_DAYS = int(os.getenv("MAX_INCREMENTAL_DAYS", "5"))
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "40"))
PAUSE_MINUTES_MIN = int(os.getenv("PAUSE_MINUTES_MIN", "30"))
PAUSE_MINUTES_MAX = int(os.getenv("PAUSE_MINUTES_MAX", "50"))

# å…¨é‡æ•°æ®èµ·å§‹æ—¥æœŸï¼ˆ2010å¹´1æœˆ1æ—¥ï¼‰
FULL_DATA_START_DATE = "20100101"

# ================== æ—¥å¿— ==================
log_dir = "./logs"
os.makedirs(log_dir, exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler(os.path.join(log_dir, "sync_ak.log"), encoding="utf-8"),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


# ================== æ ¸å¿ƒæ•°æ®å¤„ç†å‡½æ•° ==================
# ä¿®æ”¹åçš„ fetch_ak_data å‡½æ•°æ ¸å¿ƒéƒ¨åˆ†
def fetch_ak_data(code, start, end, freq="daily"):
    """ä»AKShareè·å–è‚¡ç¥¨æ•°æ®ï¼ˆæ”¯æŒæ—¥çº¿/å‘¨çº¿ï¼‰"""

    def _fetch_segment(s, e):
        @retry(
            stop=stop_after_attempt(3),
            wait=wait_exponential(multiplier=1, min=3, max=10),
            retry=retry_if_exception_type((requests.exceptions.RequestException, ConnectionError)),
            reraise=True
        )
        def _inner():
            user_agent = random.choice([
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
                "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
            ])
            original_get = requests.get

            def patched_get(url, **kwargs):
                if 'headers' not in kwargs:
                    kwargs['headers'] = {}
                kwargs['headers']['User-Agent'] = user_agent
                kwargs['proxies'] = {'http': None, 'https': None}
                return original_get(url, **kwargs)

            requests.get = patched_get
            try:
                if freq == "daily":
                    df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=s, end_date=e, adjust="qfq")
                elif freq == "weekly":
                    df = ak.stock_zh_a_hist(symbol=code, period="weekly", start_date=s, end_date=e, adjust="qfq")
                else:
                    return pd.DataFrame()

                if df.empty:
                    return df

                # âœ… å…³é”®ä¿®æ­£ï¼šæ­£ç¡®æ˜ å°„æ‰€æœ‰å­—æ®µï¼ˆåŒ…å«æˆäº¤é¢å’Œæ¢æ‰‹ç‡ï¼‰
                rename_map = {
                    'æ—¥æœŸ': 'date',
                    'å¼€ç›˜': 'open',
                    'æœ€é«˜': 'high',
                    'æœ€ä½': 'low',
                    'æ”¶ç›˜': 'close',
                    'æˆäº¤é‡': 'volume',
                    'æˆäº¤é¢': 'amount',  # âœ… æˆäº¤é¢ -> amount
                    'æ¶¨è·Œå¹…': 'pct_change',
                    'æ¶¨è·Œé¢': 'change',
                    'æ¢æ‰‹ç‡': 'turnover_rate'  # âœ… æ¢æ‰‹ç‡ -> turnover_rate
                }
                df = df.rename(columns=rename_map)[[
                    'date', 'open', 'high', 'low', 'close',
                    'volume', 'amount', 'pct_change', 'change', 'turnover_rate'
                ]].copy()

                # âœ… ç»Ÿä¸€å¤„ç†æ‰€æœ‰æ•°å€¼å­—æ®µï¼ˆå«amountå’Œturnover_rateï¼‰
                # å…ˆå°†æ‰€æœ‰å­—æ®µè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå†å¤„ç†é€—å·
                for col in ['open', 'high', 'low', 'close', 'volume', 'amount', 'pct_change', 'change',
                            'turnover_rate']:
                    # å°†éå­—ç¬¦ä¸²ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                    if df[col].dtype != 'object':
                        df[col] = df[col].astype(str)

                    # ç§»é™¤é€—å·
                    df[col] = df[col].str.replace(',', '', regex=False)

                    # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                    df[col] = pd.to_numeric(df[col], errors='coerce')

                df['code'] = code
                df['date'] = pd.to_datetime(df['date'])
                return df
            finally:
                requests.get = original_get

        return _inner()

    start_dt = datetime.strptime(start, "%Y%m%d")
    end_dt = datetime.strptime(end, "%Y%m%d")
    all_dfs = []
    current = start_dt

    while current <= end_dt:
        next_seg = current.replace(year=current.year + 2)
        seg_end = min(next_seg - timedelta(days=1), end_dt)
        s_str = current.strftime("%Y%m%d")
        e_str = seg_end.strftime("%Y%m%d")

        try:
            df_seg = _fetch_segment(s_str, e_str)
            if not df_seg.empty:
                all_dfs.append(df_seg)
        except Exception as e:
            logger.warning(f"âš ï¸ ç‰‡æ®µå¤±è´¥ ({code} {s_str}-{e_str}): {e}")
            pytime.sleep(random.uniform(1.0, 1.8))

        current = next_seg

    if all_dfs:
        result = pd.concat(all_dfs, ignore_index=True)
        result = result.drop_duplicates(subset=['date']).sort_values('date').reset_index(drop=True)
        return result
    else:
        return pd.DataFrame()


def upsert(df, table, engine, date_col):
    """æ‰¹é‡æ›´æ–°/æ’å…¥æ•°æ®ï¼ˆåˆ é™¤æ—§æ•°æ®åæ’å…¥æ–°æ•°æ®ï¼‰"""
    if df.empty:
        return

    codes = df['code'].unique().tolist()
    dates = df[date_col].dt.strftime('%Y-%m-%d').unique().tolist()

    with engine.connect() as conn:
        placeholders_c = ','.join([f"'{c}'" for c in codes])
        placeholders_d = ','.join([f"'{d}'" for d in dates])

        delete_sql = f"""
        DELETE FROM `{table}` WHERE `code` IN ({placeholders_c}) AND `{date_col}` IN ({placeholders_d})
        """
        conn.execute(text(delete_sql))
        conn.commit()

        df.to_sql(table, con=engine, if_exists='append', index=False, method='multi')


def get_latest(engine, code, table, col):
    """è·å–è¡¨ä¸­æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ"""
    try:
        with engine.connect() as conn:
            result = conn.execute(
                text(f"SELECT MAX(`{col}`) FROM `{table}` WHERE `code` = :c"),
                {"c": code}
            ).scalar()
            return result.strftime("%Y-%m-%d") if result else None
    except Exception as e:
        logger.debug(f"è·å–æœ€æ–°æ—¥æœŸå¤±è´¥ ({code}): {e}")
        return None


def create_tables(engine):
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰"""
    with engine.connect() as conn:
        # âœ… æ—¥çº¿è¡¨ï¼ˆåŒ…å«amountå’Œturnover_rateï¼‰
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS `stock_daily` (
            `id` BIGINT AUTO_INCREMENT PRIMARY KEY,
            `code` VARCHAR(20) NOT NULL,
            `date` DATE NOT NULL,
            `open` DECIMAL(10,3),
            `high` DECIMAL(10,3),
            `low` DECIMAL(10,3),
            `close` DECIMAL(10,3),
            `volume` BIGINT,
            `amount` DECIMAL(15,2),  -- âœ… æˆäº¤é¢å­—æ®µ
            `pct_change` DECIMAL(10,2),
            `change` DECIMAL(10,2),
            `turnover_rate` DECIMAL(10,2),  -- âœ… æ¢æ‰‹ç‡å­—æ®µ
            UNIQUE KEY `uk_code_date` (`code`, `date`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """))

        # âœ… å‘¨çº¿è¡¨ï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS `stock_weekly` (
            `id` BIGINT AUTO_INCREMENT PRIMARY KEY,
            `code` VARCHAR(20) NOT NULL,
            `date` DATE NOT NULL,
            `open` DECIMAL(10,3),
            `high` DECIMAL(10,3),
            `low` DECIMAL(10,3),
            `close` DECIMAL(10,3),
            `volume` BIGINT,
            `amount` DECIMAL(15,2),  -- âœ… æˆäº¤é¢
            `pct_change` DECIMAL(10,2),
            `change` DECIMAL(10,2),
            `turnover_rate` DECIMAL(10,2),  -- âœ… æ¢æ‰‹ç‡
            UNIQUE KEY `uk_code_date` (`code`, `date`)
        ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
        """))

        conn.commit()
        logger.info("âœ… æ•°æ®è¡¨å·²åˆå§‹åŒ–ï¼ˆåŒ…å«æˆäº¤é¢å’Œæ¢æ‰‹ç‡å­—æ®µï¼‰")


# ================== ä¸»é€»è¾‘ ==================
def main():
    uri = f"mysql+mysqlconnector://{MYSQL_USER}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DB}?charset=utf8mb4"
    engine = create_engine(uri, pool_pre_ping=True)
    create_tables(engine)

    try:
        all_codes = load_codes()
        if not all_codes:
            logger.warning("âš ï¸ æœªåŠ è½½åˆ°ä»»ä½•è‚¡ç¥¨ä»£ç ï¼Œè¯·æ£€æŸ¥ code.csv")
            return

        logger.info(f"å…± {len(all_codes)} åªè‚¡ç¥¨ | æ‰¹å¤§å°: {BATCH_SIZE}")

        now = datetime.now()
        end_date_str = now.strftime("%Y%m%d")

        # 15:30ååŒæ­¥å½“å¤©æ•°æ®ï¼Œå¦åˆ™åŒæ­¥å‰ä¸€å¤©
        if now.time() >= time(15, 30):
            end_date_str = now.strftime("%Y%m%d")
        else:
            end_date_str = (now - timedelta(days=1)).strftime("%Y%m%d")

        logger.info(f"åŒæ­¥æˆªæ­¢æ—¥æœŸ: {end_date_str}")

        failed_list = []
        total = len(all_codes)
        cnt = 0
        for code in all_codes:
            cnt += 1
            logger.info(f"æ­£åœ¨åŒæ­¥{cnt}/{total}")
            pytime.sleep(random.uniform(30, 60))
            try:
                # åŒæ­¥æ—¥çº¿æ•°æ®
                latest_daily = get_latest(engine, code, "stock_daily", "date")
                if latest_daily:
                    start_str = (datetime.strptime(latest_daily, "%Y-%m-%d") + timedelta(days=1)).strftime("%Y%m%d")
                else:
                    start_str = FULL_DATA_START_DATE

                df_d = fetch_ak_data(code, start_str, end_date_str, "daily")
                if not df_d.empty:
                    upsert(df_d, "stock_daily", engine, "date")

                # åŒæ­¥å‘¨çº¿æ•°æ®
                latest_weekly = get_latest(engine, code, "stock_weekly", "date")
                if latest_weekly:
                    start_str = (datetime.strptime(latest_weekly, "%Y-%m-%d") + timedelta(days=7)).strftime(
                        "%Y%m%d")
                else:
                    start_str = FULL_DATA_START_DATE

                df_w = fetch_ak_data(code, start_str, end_date_str, "weekly")
                if not df_w.empty:
                    upsert(df_w, "stock_weekly", engine, "date")

            except Exception as e:
                logger.error(f"ğŸ’¥ {code} åŒæ­¥å´©æºƒ: {e}", exc_info=True)
                failed_list.append(code)
            logger.info(f"åŒæ­¥å®Œæˆ{cnt}/{total}")

        # ä¿å­˜å¤±è´¥åˆ—è¡¨
        if failed_list:
            fail_file = os.path.join(log_dir, "failed_codes.txt")
            with open(fail_file, "w") as f:
                f.write("\n".join(failed_list))
            logger.warning(f"âŒ {len(failed_list)} åªè‚¡ç¥¨åŒæ­¥å¤±è´¥ï¼Œå·²ä¿å­˜è‡³: {fail_file}")
        else:
            logger.info("ğŸ‰ æ‰€æœ‰è‚¡ç¥¨åŒæ­¥æˆåŠŸï¼")

    except Exception as e:
        logger.exception(f"ä¸»ç¨‹åºå¼‚å¸¸: {e}")
    finally:
        logger.info("âœ… åŒæ­¥ä»»åŠ¡ç»“æŸ")


if __name__ == "__main__":
    # ä»CSVåŠ è½½è‚¡ç¥¨ä»£ç 
    def load_codes():
        df = pd.read_csv(CODE_CSV_PATH, dtype={"code": str})
        codes = []
        for c in df["code"].dropna().str.strip():
            if c:
                codes.append(c.zfill(6))
        return codes


    main()